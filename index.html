<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>AIML Lab Experiments</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      background: #f4f4f9;
      padding: 20px;
      color: #333;
    }
    h1 {
      text-align: center;
      color: #2c3e50;
    }
    details {
      background: #fff;
      border: 1px solid #ccc;
      border-radius: 8px;
      margin: 15px 0;
      padding: 10px 15px;
      box-shadow: 2px 2px 8px rgba(0,0,0,0.05);
    }
    summary {
      font-weight: bold;
      font-size: 1.1em;
      cursor: pointer;
      outline: none;
      margin-bottom: 10px;
    }
    pre {
      background: #f0f0f0;
      padding: 15px;
      overflow-x: auto;
      border-radius: 5px;
      font-size: 0.95em;
    }
    code {
      font-family: 'Courier New', monospace;
    }
  </style>
</head>
<body>
  <h1>AIML Lab Code Collection</h1>

  <details open>
    <summary>#Common for all the below codes</summary>
    <pre><code>from sklearn.datasets import load_iris
import pandas as pd
from sklearn.model_selection import train_test_split

iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</code></pre>
  </details>

  <details>
  <summary>#13 Breadth-First Search (BFS)</summary>
  <pre><code>from collections import deque

def bfs(graph, start):
    visited = set()
    queue = deque([start])
    while queue:
        vertex = queue.popleft()
        if vertex not in visited:
            print(vertex, end=" ")
            visited.add(vertex)
            queue.extend([n for n in graph[vertex] if n not in visited])

graph = {
    'A': ['B', 'C'],
    'B': ['D', 'E'],
    'C': ['F'],
    'D': [],
    'E': ['F'],
    'F': []
}

print("BFS Traversal:")
bfs(graph, 'A')</code></pre>
</details>

<details>
  <summary>#14 Dijkstra's Algorithm</summary>
  <pre><code>import heapq

def dijkstra(graph, start):
    queue = [(0, start)]
    distances = {node: float('inf') for node in graph}
    distances[start] = 0

    while queue:
        (cost, node) = heapq.heappop(queue)
        for neighbor, weight in graph[node]:
            new_cost = cost + weight
            if new_cost &lt; distances[neighbor]:
                distances[neighbor] = new_cost
                heapq.heappush(queue, (new_cost, neighbor))
    
    return distances

graph = {
    'A': [('B', 1), ('C', 4)],
    'B': [('C', 2), ('D', 5)],
    'C': [('D', 1)],
    'D': []
}

print("Shortest paths from A:")
print(dijkstra(graph, 'A'))</code></pre>
</details>

<details>
  <summary>#15 A* Search Algorithm</summary>
  <pre><code>from heapq import heappop, heappush

def a_star(graph, start, goal, h):
    open_set = [(h[start], 0, start, [])]
    visited = set()
    
    while open_set:
        est_total, cost, node, path = heappop(open_set)
        if node in visited:
            continue
        visited.add(node)
        path = path + [node]

        if node == goal:
            return path
        
        for neighbor, weight in graph.get(node, []):
            if neighbor not in visited:
                heappush(open_set, (cost + weight + h[neighbor], cost + weight, neighbor, path))
    return []

graph = {
    'A': [('B', 1), ('C', 3)],
    'B': [('D', 1)],
    'C': [('D', 1)],
    'D': []
}
heuristic = {'A': 3, 'B': 1, 'C': 1, 'D': 0}

print("A* path from A to D:")
print(a_star(graph, 'A', 'D', heuristic))</code></pre>
</details>

<details>
  <summary>#16 Memory Bounded A* (Simplified SMA*)</summary>
  <pre><code>import heapq

def sma_star(graph, start, goal, h, memory_limit=3):
    open_list = [(h[start], 0, start, [])]
    while open_list:
        open_list.sort()
        if len(open_list) > memory_limit:
            open_list.pop()
        
        est_total, cost, node, path = open_list.pop(0)
        path = path + [node]
        
        if node == goal:
            return path

        for neighbor, weight in graph.get(node, []):
            if neighbor not in path:
                heapq.heappush(open_list, (cost + weight + h[neighbor], cost + weight, neighbor, path))
    return []

graph = {
    'A': [('B', 1), ('C', 4)],
    'B': [('D', 2)],
    'C': [('D', 1)],
    'D': []
}
heuristic = {'A': 4, 'B': 2, 'C': 2, 'D': 0}

print("Memory-bounded A* path from A to D:")
print(sma_star(graph, 'A', 'D', heuristic))</code></pre>
</details>

  <details>
    <summary>#3 Naive Bayes Classifier</summary>
    <pre><code>from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

model = GaussianNB()
model.fit(X_train, y_train)
pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, pred))</code></pre>
  </details>

  <details>
    <summary>#4 Bayesian Network</summary>
    <pre><code>!pip install pgmpy
from pgmpy.models import DiscreteBayesianNetwork
from pgmpy.factors.discrete import TabularCPD
from pgmpy.inference import VariableElimination

model = DiscreteBayesianNetwork([('Glucose', 'Outcome')])
cpd_glucose = TabularCPD(variable='Glucose', variable_card=2, values=[[0.6], [0.4]])
cpd_outcome = TabularCPD(variable='Outcome', variable_card=2,
  values=[[0.7, 0.3], [0.3, 0.7]],
  evidence=['Glucose'],
  evidence_card=[2])
model.add_cpds(cpd_glucose, cpd_outcome)
assert model.check_model()
infer = VariableElimination(model)
result = infer.query(variables=['Outcome'], evidence={'Glucose': 1})

print(result)</code></pre>
  </details>

  <details>
    <summary>#5 Regression Model</summary>
    <pre><code>from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

model = LinearRegression()
model.fit(X_train, y_train)
pred = model.predict(X_test)
print("MSE:", mean_squared_error(y_test, pred))</code></pre>
  </details>

  <details>
    <summary>#6a Decision Tree</summary>
    <pre><code>from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(X_train, y_train)
print("Accuracy:", model.score(X_test, y_test))</code></pre>
  </details>

  <details>
    <summary>#7 Support Vector Machine</summary>
    <pre><code>from sklearn.svm import SVC

model = SVC(kernel='linear')
model.fit(X_train, y_train)
print("Accuracy:", model.score(X_test, y_test))</code></pre>
  </details>

  <details>
    <summary>#8 Ensemble Techniques - Random Forest</summary>
    <pre><code>from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X_train, y_train)
print("Accuracy:", model.score(X_test, y_test))</code></pre>
  </details>

  <details>
    <summary>#8 Ensemble Techniques - Gradient Boosting</summary>
    <pre><code>from sklearn.ensemble import GradientBoostingClassifier

model = GradientBoostingClassifier()
model.fit(X_train, y_train)
print("Accuracy:", model.score(X_test, y_test))</code></pre>
  </details>

  <details>
    <summary>#9 Clustering Algorithms</summary>
    <pre><code>from sklearn.cluster import KMeans, AgglomerativeClustering
import matplotlib.pyplot as plt

X_clust = X[['Glucose', 'BMI']] if 'Glucose' in X.columns else X.iloc[:, [0, 1]]
k_labels = KMeans(n_clusters=3).fit_predict(X_clust)
a_labels = AgglomerativeClustering(n_clusters=3).fit_predict(X_clust)

plt.subplot(1, 2, 1)
plt.scatter(X_clust.iloc[:, 0], X_clust.iloc[:, 1], c=k_labels)
plt.title("KMeans Clustering")

plt.subplot(1, 2, 2)
plt.scatter(X_clust.iloc[:, 0], X_clust.iloc[:, 1], c=a_labels)
plt.title("Agglomerative Clustering")
plt.show()</code></pre>
  </details>

  <details>
    <summary>#10 Expectation Maximization (GMM)</summary>
    <pre><code>from sklearn.mixture import GaussianMixture

X_clust = X[['Glucose', 'BMI']] if 'Glucose' in X.columns else X.iloc[:, [0, 1]]
model = GaussianMixture(n_components=3)
labels = model.fit_predict(X_clust)

plt.scatter(X_clust.iloc[:, 0], X_clust.iloc[:, 1], c=labels)
plt.title("EM (GMM) Clustering")
plt.show()</code></pre>
  </details>

  <details>
    <summary>#11 Simple Neural Network (Numpy)</summary>
    <pre><code>import numpy as np
X_nn = X[['Glucose', 'BMI']].values if 'Glucose' in X.columns else X.iloc[:, [0, 1]].values
y_nn = y.reshape(-1, 1)

def sigmoid(x): return 1 / (1 + np.exp(-x))
def sigmoid_deriv(x): return x * (1 - x)

np.random.seed(42)
W1 = np.random.rand(X_nn.shape[1], 3)
W2 = np.random.rand(3, 1)

for _ in range(1000):
  hidden = sigmoid(np.dot(X_nn, W1))
output = sigmoid(np.dot(hidden, W2))
error = y_nn - output
d_output = error * sigmoid_deriv(output)
d_hidden = d_output @ W2.T * sigmoid_deriv(hidden)
W2 += hidden.T @ d_output * 0.1
W1 += X_nn.T @ d_hidden * 0.1

print("Sample Output:", output[:5])</code></pre>
  </details>

  <details>
    <summary>#12 Deep Learning Neural Network (PyTorch)</summary>
    <pre><code>import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
y_tensor = torch.tensor(y.values if hasattr(y, 'values') else y, dtype=torch.long)

model = nn.Sequential(
  nn.Linear(X_tensor.shape[1], 16),
  nn.ReLU(),
  nn.Linear(16, len(set(y_tensor.numpy())))
)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

for epoch in range(10):
  optimizer.zero_grad()
  output = model(X_tensor)
  loss = criterion(output, y_tensor)
  loss.backward()
  optimizer.step()
  print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")</code></pre>
  </details>
</body>
</html>
